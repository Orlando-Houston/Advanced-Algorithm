{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.0.0-py3-none-any.whl (954 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting urllib3[secure]~=1.26\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: async-generator>=1.10 in c:\\anaconda\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.10)\n",
      "Requirement already satisfied: idna>=2.0.0; extra == \"secure\" in c:\\anaconda\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2.10)\n",
      "Requirement already satisfied: certifi; extra == \"secure\" in c:\\anaconda\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.6.20)\n",
      "Requirement already satisfied: cryptography>=1.3.4; extra == \"secure\" in c:\\anaconda\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyOpenSSL>=0.14; extra == \"secure\" in c:\\anaconda\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (19.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.14.3)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\anaconda\\lib\\site-packages (from cryptography>=1.3.4; extra == \"secure\"->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Installing collected packages: h11, wsproto, outcome, sniffio, trio, trio-websocket, urllib3, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed h11-0.12.0 outcome-1.1.0 selenium-4.0.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.7 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext\n",
    "#set some display settings for notebooks\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add headers in case you use chromedriver (captchas are no fun)\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'orlando/'\n",
    "    \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
    "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
    "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
    "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
    "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
    "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
    "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
    "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
    "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
    "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "    print(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "# page_links = [soup, soup1]\n",
    "# page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# if not test.empty:\n",
    "#     del(test)\n",
    "    \n",
    "test = pd.DataFrame()\n",
    "\n",
    "for i in soup:\n",
    "    address = list(soup.find_all (class_= 'list-card-addr'))\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    for link in soup.find_all(\"article\"):\n",
    "        href = link.find('a',class_=\"list-card-link\")\n",
    "        addresses = href.find('address')\n",
    "        addresses.extract()\n",
    "        urls.append(href)\n",
    "    links = urls\n",
    "\n",
    "#     details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "#     home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "#     last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "#     brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    \n",
    "    test['prices'] = price\n",
    "    test['address'] = address\n",
    "    test['beds'] = beds\n",
    "    test['beds'] = test['beds'].astype('str')\n",
    "    test['links'] = pd.Series(links)\n",
    "    \n",
    "    test['beds'] = test['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "    test['beds'] = test['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "    test['beds'] = test['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "    test['beds'] = test['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "    test['beds'] = test['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "    \n",
    "    test[['beds','baths','sq_feet']] = test.beds.str.split(expand=True)\n",
    "    \n",
    "#     df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "#     df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "#     df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "# links\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original\n",
    "#     urls = []\n",
    "#     for link in soup.find_all(\"article\"):\n",
    "#         href = link.find('a',class_=\"list-card-link\")\n",
    "#         if \"address\" in href.text:\n",
    "#             addresses = href.find('address')\n",
    "#             addresses.extract()\n",
    "#         else:\n",
    "#             pass\n",
    "#         urls.append(href) \n",
    "\n",
    "# for link in soup.find_all(\"article\"):\n",
    "#     href = link.find('a',class_=\"list-card-link\")\n",
    "#     if \"address\" in href.text:\n",
    "#         addresses = href.find('a','address')\n",
    "#         addresses.extract()\n",
    "#     else:\n",
    "#         pass\n",
    "#     urls.append(href)\n",
    "\n",
    "# links = soup.find_all(class_=\"list-card-link\")\n",
    "# print(links)\n",
    "#      if \"tabindex\" in link.text:\n",
    "#         print(link)\n",
    "#         print(link.attrs['href'])\n",
    "# del(soup)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "#create links df and empty url list\n",
    "links_df = pd.DataFrame()\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "links_df['links'] = urls\n",
    "links_df['links'] = links_df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "links_df['links'] = links_df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "links_df['links'] = links_df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ANACONDA\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-362c6de56c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mhref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"list-card-link\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0maddresses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'address'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0maddresses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "\n",
    "#set some display settings for notebooks\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "     'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'\n",
    "}\n",
    "\n",
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'austin/'\n",
    "    \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
    "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
    "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
    "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
    "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
    "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
    "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
    "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
    "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
    "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "\n",
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup1.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append first two dataframes\n",
    "df = df.append(df1, ignore_index = True) \n",
    "\n",
    "#create empty dataframes\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "df7 = pd.DataFrame()\n",
    "df8 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "for i in soup2:\n",
    "    soup = soup2\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df2['prices'] = price\n",
    "    df2['address'] = address\n",
    "    df2['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup2.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df2['links'] = urls\n",
    "df2['links'] = df2['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "    \n",
    "for i in soup3:\n",
    "    soup = soup3\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df3['prices'] = price1\n",
    "    df3['address'] = address1\n",
    "    df3['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup3.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df3['links'] = urls\n",
    "df3['links'] = df3['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup4:\n",
    "    soup = soup4\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df4['prices'] = price1\n",
    "    df4['address'] = address1\n",
    "    df4['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup4.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df4['links'] = urls\n",
    "df4['links'] = df4['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup5:\n",
    "    soup = soup5\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df5['prices'] = price1\n",
    "    df5['address'] = address1\n",
    "    df5['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup5.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df5['links'] = urls\n",
    "df5['links'] = df5['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup6:\n",
    "    soup = soup6\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df6['prices'] = price1\n",
    "    df6['address'] = address1\n",
    "    df6['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup6.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df6['links'] = urls\n",
    "df6['links'] = df6['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup7:\n",
    "    soup = soup7\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df7['prices'] = price1\n",
    "    df7['address'] = address1\n",
    "    df7['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup7.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df7['links'] = urls\n",
    "df7['links'] = df7['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup8:\n",
    "    soup = soup8\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df8['prices'] = price1\n",
    "    df8['address'] = address1\n",
    "    df8['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup8.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df8['links'] = urls\n",
    "df8['links'] = df8['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup9:\n",
    "    soup = soup9\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df9['prices'] = price1\n",
    "    df9['address'] = address1\n",
    "    df9['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup9.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df9['links'] = urls\n",
    "df9['links'] = df9['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "df = df.append(df2, ignore_index = True) \n",
    "df = df.append(df3, ignore_index = True) \n",
    "df = df.append(df4, ignore_index = True) \n",
    "df = df.append(df5, ignore_index = True) \n",
    "df = df.append(df6, ignore_index = True) \n",
    "df = df.append(df7, ignore_index = True) \n",
    "df = df.append(df8, ignore_index = True) \n",
    "df = df.append(df9, ignore_index = True) \n",
    "\n",
    "#convert columns to str\n",
    "df['prices'] = df['prices'].astype('str')\n",
    "df['address'] = df['address'].astype('str')\n",
    "df['beds'] = df['beds'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
    "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "#remove html tags from beds column\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
    "\n",
    "#split beds column into beds, bath and sq_feet\n",
    "df[['beds','baths','sq_feet']] = df.beds.str.split(expand=True)\n",
    "\n",
    "#remove commas from sq_feet and convert to float\n",
    "df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "#drop nulls\n",
    "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
    "\n",
    "#convert column to float\n",
    "df['prices'] = df['prices'].astype('float')\n",
    "# df['sq_feet'] = df['sq_feet'].astype('float')\n",
    "\n",
    "print('The column datatypes are:')\n",
    "print(df.dtypes)\n",
    "print('The dataframe shape is:', df.shape)\n",
    "\n",
    "#rearrange the columns\n",
    "df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'links'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'links'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-da9e311bdc72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# zpid_test = pd.DataFrame()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'links'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msearched_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"contactPhone\":\"\",\"zestimate\":'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreq_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'links'"
     ]
    }
   ],
   "source": [
    "# zpid_test = pd.DataFrame()\n",
    "\n",
    "for link in df['links']:\n",
    "    searched_word = '\"contactPhone\":\"\",\"zestimate\":'\n",
    "    test_page = requests.Session().get(link, headers=req_headers)\n",
    "    test_soup = BeautifulSoup(test_page.content, 'lxml')\n",
    "    results = test_soup.body.find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)\n",
    "    print(results)[0]\n",
    "#     zpid = test_soup.find_all('div')\n",
    "#     text = test_soup.get_text()\n",
    "#     print(text)[0]\n",
    "#     print(test_soup)[0]\n",
    "#     print(zpid)[0]\n",
    "\n",
    "    \n",
    "# zpid_test['zpid'] = zpid\n",
    "# zpid\n",
    "#     print(link)\n",
    "#     urls = []\n",
    "#     for link in soup.find_all(\"article\"):\n",
    "#         href = link.find('a',class_=\"list-card-link\")\n",
    "#         if \"address\" in href.text:\n",
    "#             addresses = href.find('address')\n",
    "#             addresses.extract()\n",
    "#         else:\n",
    "#             pass\n",
    "#         urls.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sq_feet', 'links', 'baths'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a723a12d89fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prices'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'address'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'links'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'beds'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'baths'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sq_feet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2906\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2908\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;31m# we skip the warning on Categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['sq_feet', 'links', 'baths'] not in index\""
     ]
    }
   ],
   "source": [
    "df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['beds1'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6625ff7461e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'beds1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5282\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['beds1'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = df.drop(['beds1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_house_links(url, driver, pages=20):\n",
    "    house_links=[]\n",
    "    driver.get(url)\n",
    "    for i in range(pages):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        listings = soup.find_all(\"a\", class_=\"zsg-photo-card-overlay-link\")\n",
    "        page_data = ['https://www.zillow.com'+row['href'] for row in listings]\n",
    "        house_links.append(page_data)\n",
    "        time.sleep(np.random.lognormal(0,1))\n",
    "        next_button = soup.find_all(\"a\", title=\"Next page\")\n",
    "        next_button_link = ['https://www.zillow.com/homes/for_sale/orlando,-tx_rb'+row['href'] for row in next_button]\n",
    "        if i<19:\n",
    "            driver.get(next_button_link[0])\n",
    "    \n",
    "    return house_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.zillow.com/homes/for_sale/orlando,-tx_rb/1_p//austin-tx/2_p/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(url)\n",
    "n = '1'\n",
    "city = 'orlando,-tx_rb/'\n",
    "url = 'https://www.zillow.com/homes/for_sale/'+city+n+'_p/'\n",
    "next_button = soup.find_all(\"a\", title=\"Next page\")\n",
    "next_button_link = [url+row['href'] for row in next_button]\n",
    "next_button_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
