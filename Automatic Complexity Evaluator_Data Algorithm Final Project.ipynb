{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational Complexity of ML Algorithms\n",
    "# https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/\n",
    "#https://www.semanticscholar.org/paper/ACE%3A-an-automatic-complexity-evaluator-M%C3%A9tayer/40fddae949cc89c4cc34562104e765ee301483da\n",
    "# https://github.com/terryyin/lizard\n",
    "# https://medium.com/analytics-vidhya/time-complexity-of-ml-models-4ec39fad2770\n",
    "# https://medium.com/analytics-vidhya/computational-complexity-of-ml-algorithms-1bdc88af1c7a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A theoretical point of view\n",
    "* Some bounds\n",
    "* Here, I propose upper bounds (as the implementation achieving this bound will be described) when the data is dense.\n",
    "\n",
    "* Calling nn the number of training sample, pp the number of features, ntreesntrees the number of trees (for methods based on various trees), nsvnsv, the number of support vectors and nlinli the number of neurons at layer ii in a neural network, we have the following approximations.\n",
    "* Algorithm\t     Classification/Regression\t                     Training\t                  Prediction\n",
    "* Decision Tree\t          C+R\t                                O(n2p)O(n2p)\t                O(p)O(p)\n",
    "* Random Forest\t          C+R\t                            O(n2pntrees)O(n2pntrees)  \tO(pntrees)O(pntrees)\n",
    "* Random Forest\tR Breiman implementation\t                O(n2pntrees)O(n2pntrees)\tO(pntrees)O(pntrees)\n",
    "* Random Forest\tC Breiman implementation\t                  O(n2√pntrees)O(n2pntrees)\tO(pntrees)O(pntrees)\n",
    "* Extremly Random Trees\t C+R\t                              O(npntrees)O(npntrees)\tO(npntrees)O(npntrees)\n",
    "* Gradient Boosting (ntreesntrees)\tC+R\tO(npntrees)O(npntrees)\tO(pntrees)O(pntrees)\n",
    "* Linear Regression\tR\t                                        O(p2n+p3)O(p2n+p3)\t            O(p)O(p)\n",
    "* SVM (Kernel)\tC+R\t                                             O(n2p+n3)O(n2p+n3)\t          O(nsvp)O(nsvp)\n",
    "* k-Nearest Neighbours (naive)\tC+R\t−−\t                                O(np)                      O(np)\n",
    "* Nearest centroid\tC\t                                               O(np)O(np)\t              O(p)O(p)\n",
    "* Neural Network\tC+R\t ?\t                                           O(pnl1+nl1nl2+...)O(pnl1+nl1nl2+...)\n",
    "* Naive Bayes\t     C\t                                                   O(np)O(np)\t      O(p)\n",
    "\n",
    "* Justifications\n",
    "* Decision Tree based models\n",
    "* Obviously, ensemble methods multiply the complexity of the original model by the number of “voters” in the model, and replace the training size by the size of each bag.\n",
    "* When training a decision tree, a split has to be found until a maximum depth dd has been reached.\n",
    "* The strategy for finding this split is to look for each variable (there are pp of them) to the different thresholds (there are up to nn of them) and the information gain that is achieved (evaluation in O(n)O(n))\n",
    "* In the Breiman implementation, and for classification, it is recommanded to use √pp predictors for each (weak) classifier.\n",
    "* Extremly random trees\n",
    "* The search strategy for the optimal split simply does not take place in the case of ERTs. This makes it much simpler to find a (weaker) split.\n",
    "* However (in my experience), ERTs implementation are not much faster than RFs.\n",
    "* Linear regressions\n",
    "* The problem of finding the vector of weights ββ in a linear regression boils down to evaluating the following equation: β=(X′X)−1X′Yβ=(X′X)−1X′Y.\n",
    "* The most computationnaly intensive part is to evaluate the product X′XX′X, which is done in p2np2n operations, and then inverting it, which is done in p3p3 operations.\n",
    "* Though most implementations prefer to use a gradient descent to solve the system of equations (X′X)β=X′Y(X′X)β=X′Y, the complexity remains the same.\n",
    "* Support Vector Machine\n",
    "* For the training part, the classical algorithms require to evaluate the kernel matrix KK, the matrix whose general term is K(xi,xj)K(xi,xj) where KK is the specified kernel.\n",
    "* It is assumed that K can be evaluated with a O(p)O(p) complexity, as it is true for common kernels (Gaussian, polynomials, sigmoid…). This assumption may be wrong for other kernels.\n",
    "* Then, solving the constrained quadratic programm is “morally equivalent to” inverting a square matrix of size nn, whose complexity is assumed to be O(n3)O(n3)\n",
    "* k-Nearest Neighbours\n",
    "* In its simplest form, given a new data point xx, the kNN algorithm looks for the k closest points to xx in the training data and returns the most common label (or the averaged values of targets for a regression problem).\n",
    "* To achieve this, it is necessary to compare the distance between xx and every point in the data set. This amounts to nn operations. For the common distances (Euclide, Manhattan…) this operation is performed in a O(p)O(p) operations. Not that kernel k Nearest Neighbours have the same complexity (provided the kernels enjoy the same property).\n",
    "* However, many efforts pre-train the kNN, indexing the points with quadtrees, which enable to lower dramatically the number of comparisons to the points in the training set.\n",
    "* Likewise, in the case of a sparse data set, with an inverted indexation of the rows, it is not necessary to compute all the pairwise distances.\n",
    "* The practical point of view\n",
    "* All this is nice, but what about real life examples ? We can focus on sk-learn implementations below.\n",
    "* The assumptions will be that the complexities take the form of O(nαpβ)O(nαpβ) and αα and ββ will be estimated using randomly generated samples with nn and pp varying. Then, using a log-log regression, the complexities are estimated.\n",
    "* Though this assumption is wrong, it should help to have a better idea of how the algorithms work and it will reveal some implementation details / difference between the default settings of the same algorithm that one may overlook.\n",
    "* The results\n",
    "* Method\t                  αα\tββ\n",
    "* RandomForestRegressor\t    1.21\t0.89\n",
    "* ExtraTreesRegressor\t     1.03\t0.88\n",
    "* AdaBoostRegressor\t        0.71\t1.01\n",
    "* LinearRegression\t        0.72\t1.3\n",
    "* SVR\t                    1.96\t0.42\n",
    "* RandomForestClassifier\t1.09\t0.38\n",
    "* ExtraTreesClassifier\t    0.81\t0.31\n",
    "* AdaBoostClassifier\t    0.89\t0.79\n",
    "* SVC\t                    2.05\t0.52\n",
    "* LogisticRegression(solver=liblinear)\t0.9\t 0.88\n",
    "* LogisticRegression(solver=sag)\t       1.03\t 0.95\n",
    "* Surprisingly, some methods are sublinear in nn. Perhaps the sample sizes were too small. As expected, the Support Vector show a complexity in nn that does not scale well with the sample size (close to 2).\n",
    "* Another interesting point to note are the complexities in pp for the random forest and extra trees, the component in pp varies according to the fact that we are performing a regression or a classification problem. A short look at the documentation explains it, they have different behaviors for each problem!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code\n",
    "Fore those who would like to run the code over other algorithms, here is the method I used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features : int, float, string or None, optional (default=”auto”)\n",
    "\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "If int, then consider max_features features at each split.\n",
    "If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "If “auto”, then max_features=n_features.\n",
    "If “sqrt”, then max_features=sqrt(n_features).\n",
    "If “log2”, then max_features=log2(n_features).\n",
    "If None, then max_features=n_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(\"../input/ml-cplexity/ml.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ComplexityEvaluator:\n",
    "\n",
    "    def __init__(self, nrow_samples, ncol_samples):\n",
    "        self._nrow_samples = nrow_samples\n",
    "        self._ncol_samples = ncol_samples\n",
    "\n",
    "    def _time_samples(self, model, random_data_generator):\n",
    "        rows_list = []\n",
    "        for nrow in self._nrow_samples:\n",
    "            for ncol in self._ncol_samples:\n",
    "                train, labels = random_data_generator(nrow, ncol)\n",
    "\n",
    "                start_time = time.time()\n",
    "                model.fit(train, labels)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                result = {\"N\": nrow, \"P\": ncol, \"Time\": elapsed_time}\n",
    "                rows_list.append(result)\n",
    "\n",
    "        return rows_list\n",
    "\n",
    "    def Run(self, model, random_data_generator):\n",
    "        data = pd.DataFrame(self._time_samples(model, random_data_generator))\n",
    "        print(data)\n",
    "        data = data.applymap(math.log)\n",
    "        linear_model = LinearRegression(fit_intercept=True)\n",
    "        linear_model.fit(data[[\"N\", \"P\"]], data[[\"Time\"]])\n",
    "        return linear_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self , x, y):\n",
    "        time.sleep(x.shape[0] /1000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data_generator(n , p):\n",
    "    return np.random.rand(n , p) , np.random.rand(n , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a small unit test, everything seems consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       N   P      Time\n",
      "0    200   1  0.211877\n",
      "1    200   5  0.204793\n",
      "2    200  10  0.205135\n",
      "3    500   1  0.502595\n",
      "4    500   5  0.505322\n",
      "5    500  10  0.503024\n",
      "6   1000   1  1.003017\n",
      "7   1000   5  1.007673\n",
      "8   1000  10  1.010750\n",
      "9   2000   1  2.015588\n",
      "10  2000   5  2.006563\n",
      "11  2000  10  2.014557\n",
      "12  3000   1  3.010454\n",
      "13  3000   5  3.007248\n",
      "14  3000  10  3.009346\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = TestModel()\n",
    "    nrow_samples = [200, 500, 1000, 2000, 3000]\n",
    "    ncol_samples = [1,5,10]\n",
    "    complexity_evaluator = ComplexityEvaluator(nrow_samples , ncol_samples)\n",
    "    res = complexity_evaluator.Run(model , random_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_models = [RandomForestRegressor(),\n",
    "                     ExtraTreesRegressor(),\n",
    "                     AdaBoostRegressor(),\n",
    "                     LinearRegression(),\n",
    "                     SVR()]\n",
    "\n",
    "classification_models = [RandomForestClassifier(),\n",
    "                         ExtraTreesClassifier(),\n",
    "                         AdaBoostClassifier(),\n",
    "                         SVC(),\n",
    "                         LogisticRegression(),\n",
    "                         LogisticRegression(solver='sag')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"RandomForestRegressor\",\n",
    "         \"ExtraTreesRegressor\",\n",
    "         \"AdaBoostRegressor\",\n",
    "         \"LinearRegression\",\n",
    "         \"SVR\",\n",
    "         \"RandomForestClassifier\",\n",
    "         \"ExtraTreesClassifier\",\n",
    "         \"AdaBoostClassifier\",\n",
    "         \"SVC\",\n",
    "         \"LogisticRegression(solver=liblinear)\",\n",
    "         \"LogisticRegression(solver=sag)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/aozde/OneDrive/Documents/CapstoneProject/HouseData/Sample project/kc_house_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e5de22cff5d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#using sample data to run on different models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/aozde/OneDrive/Documents/CapstoneProject/HouseData/Sample project/kc_house_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[1;33m!=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/aozde/OneDrive/Documents/CapstoneProject/HouseData/Sample project/kc_house_data.csv'"
     ]
    }
   ],
   "source": [
    "#using sample data to run on different models\n",
    "sample_data = pd.read_csv('C:/Users/aozde/OneDrive/Documents/CapstoneProject/HouseData/Sample project/kc_house_data.csv')\n",
    "sample_data = sample_data.loc[:, sample_data.dtypes !=np.object]\n",
    "sample_data = sample_data.fillna(0)\n",
    "nrows = sample_data.iloc[:,:-1].values.tolist()\n",
    "ncols = sample_data['price'].values.tolist()\n",
    "complexity_evaluator = ComplexityEvaluator(nrows,ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Run() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-9acc18ebb844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mregression_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomplexity_evaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_data_generator\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'House price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     print(names[i] + ' | ' + str(round(res[0], 2)) +\n\u001b[0;32m      5\u001b[0m           ' | ' + str(round(res[1], 2)))\n",
      "\u001b[1;31mTypeError\u001b[0m: Run() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for model in regression_models:\n",
    "    res = complexity_evaluator.Run(model, random_data_generator , 'House price')[0]\n",
    "    print(names[i] + ' | ' + str(round(res[0], 2)) +\n",
    "          ' | ' + str(round(res[1], 2)))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And, some unit tests (that can just be appended at the bottom of the previous class).\n",
    "### After a small unit test, everything seems consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityEvaluator:\n",
    "\n",
    "    def __init__(self, nrow_samples, ncol_samples):\n",
    "        self._nrow_samples = nrow_samples\n",
    "        self._ncol_samples = ncol_samples\n",
    "\n",
    "    def _time_samples(self, model, random_data_generator):\n",
    "        rows_list = []\n",
    "        for nrow in self._nrow_samples:\n",
    "            for ncol in self._ncol_samples:\n",
    "                train, labels = random_data_generator(nrow, ncol)\n",
    "\n",
    "                start_time = time.time()\n",
    "                model.fit(train, labels)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                result = {\"N\": nrow, \"P\": ncol, \"Time\": elapsed_time}\n",
    "                rows_list.append(result)\n",
    "\n",
    "        return rows_list\n",
    "\n",
    "    def Run(self, model, random_data_generator):\n",
    "        data = pd.DataFrame(self._time_samples(model, random_data_generator))\n",
    "        print(data)\n",
    "        data = data.applymap(math.log)\n",
    "        linear_model = LinearRegression(fit_intercept=True)\n",
    "        linear_model.fit(data[[\"N\", \"P\"]], data[[\"Time\"]])\n",
    "        return linear_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       N   P      Time\n",
      "0    200   1  0.214533\n",
      "1    200   5  0.205450\n",
      "2    200  10  0.202715\n",
      "3    500   1  0.505632\n",
      "4    500   5  0.508802\n",
      "5    500  10  0.505620\n",
      "6   1000   1  1.014447\n",
      "7   1000   5  1.011487\n",
      "8   1000  10  1.014487\n",
      "9   2000   1  2.012260\n",
      "10  2000   5  2.005218\n",
      "11  2000  10  2.012006\n",
      "12  3000   1  3.011629\n",
      "13  3000   5  3.013196\n",
      "14  3000  10  3.001961\n",
      "[[ 0.9884102  -0.00523214]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    class TestModel:\n",
    "\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def fit(self, x, y):\n",
    "            time.sleep(x.shape[0] / 1000.)\n",
    "\n",
    "    def random_data_generator(n, p):\n",
    "        return np.random.rand(n, p), np.random.rand(n, 1)\n",
    "\n",
    "    model = TestModel()\n",
    "\n",
    "    complexity_evaluator = ComplexityEvaluator(\n",
    "            [200, 500, 1000, 2000, 3000], [1,5,10])\n",
    "\n",
    "    res = complexity_evaluator.Run(model, random_data_generator)\n",
    "\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ComplexityEvaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-60c854682296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mComplexityEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ComplexityEvaluator'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ComplexityEvaluator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def random_data_regression(n, p):\n",
    "    return np.random.rand(n, p), np.random.rand(n)\n",
    "\n",
    "\n",
    "def random_data_classification(n, p):\n",
    "    return np.random.rand(n, p), np.random.binomial(1, 0.5, n)\n",
    "\n",
    "\n",
    "regression_models = [RandomForestRegressor(),\n",
    "                     ExtraTreesRegressor(),\n",
    "                     AdaBoostRegressor(),\n",
    "                     LinearRegression(),\n",
    "                     SVR()]\n",
    "\n",
    "classification_models = [RandomForestClassifier(),\n",
    "                         ExtraTreesClassifier(),\n",
    "                         AdaBoostClassifier(),\n",
    "                         SVC(),\n",
    "                         LogisticRegression(),\n",
    "                         LogisticRegression(solver='sag')]\n",
    "\n",
    "names = [\"RandomForestRegressor\",\n",
    "         \"ExtraTreesRegressor\",\n",
    "         \"AdaBoostRegressor\",\n",
    "         \"LinearRegression\",\n",
    "         \"SVR\",\n",
    "         \"RandomForestClassifier\",\n",
    "         \"ExtraTreesClassifier\",\n",
    "         \"AdaBoostClassifier\",\n",
    "         \"SVC\",\n",
    "         \"LogisticRegression(solver=liblinear)\",\n",
    "         \"LogisticRegression(solver=sag)\"]\n",
    "\n",
    "complexity_evaluator = ComplexityEvaluator.ComplexityEvaluator(\n",
    "    [500, 1000, 2000, 5000, 10000, 15000, 20000],\n",
    "    [5, 10, 20, 50, 100, 200])\n",
    "\n",
    "i = 0\n",
    "for model in regression_models:\n",
    "    res = complexity_evaluator.Run(model, random_data_regression)[0]\n",
    "    print(names[i] + ' | ' + str(round(res[0], 2)) +\n",
    "          ' | ' + str(round(res[1], 2)))\n",
    "    i = i + 1\n",
    "\n",
    "for model in classification_models:\n",
    "    res = complexity_evaluator.Run(model, random_data_classification)[0]\n",
    "    print(names[i] + ' | ' + str(round(res[0], 2)) +\n",
    "          ' | ' + str(round(res[1], 2)))\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  So let’s enjoy the number of algorithms offered by sklearn. The following list may be updated as new algorithms are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ComplexityEvaluator (from versions: none)\n",
      "ERROR: No matching distribution found for ComplexityEvaluator\n"
     ]
    }
   ],
   "source": [
    "!pip install ComplexityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ComplexityEvaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-60c854682296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mComplexityEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ComplexityEvaluator'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ComplexityEvaluator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def random_data_regression(n, p):\n",
    "    return np.random.rand(n, p), np.random.rand(n)\n",
    "\n",
    "\n",
    "def random_data_classification(n, p):\n",
    "    return np.random.rand(n, p), np.random.binomial(1, 0.5, n)\n",
    "\n",
    "\n",
    "regression_models = [RandomForestRegressor(),\n",
    "                     ExtraTreesRegressor(),\n",
    "                     AdaBoostRegressor(),\n",
    "                     LinearRegression(),\n",
    "                     SVR()]\n",
    "\n",
    "classification_models = [RandomForestClassifier(),\n",
    "                         ExtraTreesClassifier(),\n",
    "                         AdaBoostClassifier(),\n",
    "                         SVC(),\n",
    "                         LogisticRegression(),\n",
    "                         LogisticRegression(solver='sag')]\n",
    "\n",
    "names = [\"RandomForestRegressor\",\n",
    "         \"ExtraTreesRegressor\",\n",
    "         \"AdaBoostRegressor\",\n",
    "         \"LinearRegression\",\n",
    "         \"SVR\",\n",
    "         \"RandomForestClassifier\",\n",
    "         \"ExtraTreesClassifier\",\n",
    "         \"AdaBoostClassifier\",\n",
    "         \"SVC\",\n",
    "         \"LogisticRegression(solver=liblinear)\",\n",
    "         \"LogisticRegression(solver=sag)\"]\n",
    "\n",
    "complexity_evaluator = ComplexityEvaluator.ComplexityEvaluator(\n",
    "    [500, 1000, 2000, 5000, 10000, 15000, 20000],\n",
    "    [5, 10, 20, 50, 100, 200])\n",
    "\n",
    "i = 0\n",
    "for model in regression_models:\n",
    "    res = complexity_evaluator.Run(model, random_data_regression)[0]\n",
    "    print(names[i] + ' | ' + str(round(res[0], 2)) +\n",
    "          ' | ' + str(round(res[1], 2)))\n",
    "    i = i + 1\n",
    "\n",
    "for model in classification_models:\n",
    "    res = complexity_evaluator.Run(model, random_data_classification)[0]\n",
    "    print(names[i] + ' | ' + str(round(res[0], 2)) +\n",
    "          ' | ' + str(round(res[1], 2)))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/speed-up-jupyter-notebooks-20716cbe2025\n",
    "# https://towardsdatascience.com/understanding-time-complexity-with-python-examples-2bda6e8158a7\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html\n",
    "# https://people.duke.edu/~ccc14/sta-663/AlgorithmicComplexity.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
